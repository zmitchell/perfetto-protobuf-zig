// =============================================================================
// DO NOT EDIT - This file is automatically generated by gremlin.zig
// =============================================================================
const std = @import("std");
const gremlin = @import("gremlin");
const process_metadata = @import("process_metadata.proto.zig");
// structs
const JavaHeapHistogramWire = struct {
    const INSTANCE_STATS_WIRE: gremlin.ProtoWireNumber = 1;
};
pub const JavaHeapHistogram = struct {
    // nested structs
    const TypeCountWire = struct {
        const TYPE_NAME_WIRE: gremlin.ProtoWireNumber = 1;
        const CATEGORY_WIRE: gremlin.ProtoWireNumber = 4;
        const OBJ_COUNT_WIRE: gremlin.ProtoWireNumber = 2;
        const REACHABLE_OBJ_COUNT_WIRE: gremlin.ProtoWireNumber = 3;
        const SIZE_KB_WIRE: gremlin.ProtoWireNumber = 5;
        const REACHABLE_SIZE_KB_WIRE: gremlin.ProtoWireNumber = 6;
        const NATIVE_SIZE_KB_WIRE: gremlin.ProtoWireNumber = 7;
        const REACHABLE_NATIVE_SIZE_KB_WIRE: gremlin.ProtoWireNumber = 8;
    };
    pub const TypeCount = struct {
        // fields
        type_name: ?[]const u8 = null,
        category: ?[]const u8 = null,
        obj_count: u32 = 0,
        reachable_obj_count: u32 = 0,
        size_kb: u32 = 0,
        reachable_size_kb: u32 = 0,
        native_size_kb: u32 = 0,
        reachable_native_size_kb: u32 = 0,
        pub fn calcProtobufSize(self: *const JavaHeapHistogram.TypeCount) usize {
            var res: usize = 0;
            if (self.type_name) |v| {
                if (v.len > 0) {
                    res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.TYPE_NAME_WIRE) + gremlin.sizes.sizeUsize(v.len) + v.len;
                }
            }
            if (self.category) |v| {
                if (v.len > 0) {
                    res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.CATEGORY_WIRE) + gremlin.sizes.sizeUsize(v.len) + v.len;
                }
            }
            if (self.obj_count != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.OBJ_COUNT_WIRE) + gremlin.sizes.sizeU32(self.obj_count);
            }
            if (self.reachable_obj_count != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.REACHABLE_OBJ_COUNT_WIRE) + gremlin.sizes.sizeU32(self.reachable_obj_count);
            }
            if (self.size_kb != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.SIZE_KB_WIRE) + gremlin.sizes.sizeU32(self.size_kb);
            }
            if (self.reachable_size_kb != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.REACHABLE_SIZE_KB_WIRE) + gremlin.sizes.sizeU32(self.reachable_size_kb);
            }
            if (self.native_size_kb != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.NATIVE_SIZE_KB_WIRE) + gremlin.sizes.sizeU32(self.native_size_kb);
            }
            if (self.reachable_native_size_kb != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.TypeCountWire.REACHABLE_NATIVE_SIZE_KB_WIRE) + gremlin.sizes.sizeU32(self.reachable_native_size_kb);
            }
            return res;
        }
        pub fn encode(self: *const JavaHeapHistogram.TypeCount, allocator: std.mem.Allocator) gremlin.Error![]const u8 {
            const size = self.calcProtobufSize();
            if (size == 0) {
                return &[_]u8{};
            }
            const buf = try allocator.alloc(u8, self.calcProtobufSize());
            var writer = gremlin.Writer.init(buf);
            self.encodeTo(&writer);
            return buf;
        }
        pub fn encodeTo(self: *const JavaHeapHistogram.TypeCount, target: *gremlin.Writer) void {
            if (self.type_name) |v| {
                if (v.len > 0) {
                    target.appendBytes(JavaHeapHistogram.TypeCountWire.TYPE_NAME_WIRE, v);
                }
            }
            if (self.category) |v| {
                if (v.len > 0) {
                    target.appendBytes(JavaHeapHistogram.TypeCountWire.CATEGORY_WIRE, v);
                }
            }
            if (self.obj_count != 0) {
                target.appendUint32(JavaHeapHistogram.TypeCountWire.OBJ_COUNT_WIRE, self.obj_count);
            }
            if (self.reachable_obj_count != 0) {
                target.appendUint32(JavaHeapHistogram.TypeCountWire.REACHABLE_OBJ_COUNT_WIRE, self.reachable_obj_count);
            }
            if (self.size_kb != 0) {
                target.appendUint32(JavaHeapHistogram.TypeCountWire.SIZE_KB_WIRE, self.size_kb);
            }
            if (self.reachable_size_kb != 0) {
                target.appendUint32(JavaHeapHistogram.TypeCountWire.REACHABLE_SIZE_KB_WIRE, self.reachable_size_kb);
            }
            if (self.native_size_kb != 0) {
                target.appendUint32(JavaHeapHistogram.TypeCountWire.NATIVE_SIZE_KB_WIRE, self.native_size_kb);
            }
            if (self.reachable_native_size_kb != 0) {
                target.appendUint32(JavaHeapHistogram.TypeCountWire.REACHABLE_NATIVE_SIZE_KB_WIRE, self.reachable_native_size_kb);
            }
        }
    };
    pub const TypeCountReader = struct {
        buf: gremlin.Reader,
        _type_name: ?[]const u8 = null,
        _category: ?[]const u8 = null,
        _obj_count: u32 = 0,
        _reachable_obj_count: u32 = 0,
        _size_kb: u32 = 0,
        _reachable_size_kb: u32 = 0,
        _native_size_kb: u32 = 0,
        _reachable_native_size_kb: u32 = 0,
        pub fn init(src: []const u8) gremlin.Error!JavaHeapHistogram.TypeCountReader {
            const buf = gremlin.Reader.init(src);
            var res = JavaHeapHistogram.TypeCountReader{ .buf = buf };
            if (buf.buf.len == 0) {
                return res;
            }
            var offset: usize = 0;
            while (buf.hasNext(offset, 0)) {
                const tag = try buf.readTagAt(offset);
                offset += tag.size;
                switch (tag.number) {
                    JavaHeapHistogram.TypeCountWire.TYPE_NAME_WIRE => {
                        const result = try buf.readBytes(offset);
                        offset += result.size;
                        res._type_name = result.value;
                    },
                    JavaHeapHistogram.TypeCountWire.CATEGORY_WIRE => {
                        const result = try buf.readBytes(offset);
                        offset += result.size;
                        res._category = result.value;
                    },
                    JavaHeapHistogram.TypeCountWire.OBJ_COUNT_WIRE => {
                        const result = try buf.readUInt32(offset);
                        offset += result.size;
                        res._obj_count = result.value;
                    },
                    JavaHeapHistogram.TypeCountWire.REACHABLE_OBJ_COUNT_WIRE => {
                        const result = try buf.readUInt32(offset);
                        offset += result.size;
                        res._reachable_obj_count = result.value;
                    },
                    JavaHeapHistogram.TypeCountWire.SIZE_KB_WIRE => {
                        const result = try buf.readUInt32(offset);
                        offset += result.size;
                        res._size_kb = result.value;
                    },
                    JavaHeapHistogram.TypeCountWire.REACHABLE_SIZE_KB_WIRE => {
                        const result = try buf.readUInt32(offset);
                        offset += result.size;
                        res._reachable_size_kb = result.value;
                    },
                    JavaHeapHistogram.TypeCountWire.NATIVE_SIZE_KB_WIRE => {
                        const result = try buf.readUInt32(offset);
                        offset += result.size;
                        res._native_size_kb = result.value;
                    },
                    JavaHeapHistogram.TypeCountWire.REACHABLE_NATIVE_SIZE_KB_WIRE => {
                        const result = try buf.readUInt32(offset);
                        offset += result.size;
                        res._reachable_native_size_kb = result.value;
                    },
                    else => {
                        offset = try buf.skipData(offset, tag.wire);
                    },
                }
            }
            return res;
        }
        pub fn sourceBytes(self: *const @This()) []const u8 {
            return self.buf.buf;
        }
        pub inline fn getTypeName(self: *const JavaHeapHistogram.TypeCountReader) []const u8 {
            return self._type_name orelse &[_]u8{};
        }
        pub inline fn getCategory(self: *const JavaHeapHistogram.TypeCountReader) []const u8 {
            return self._category orelse &[_]u8{};
        }
        pub inline fn getObjCount(self: *const JavaHeapHistogram.TypeCountReader) u32 {
            return self._obj_count;
        }
        pub inline fn getReachableObjCount(self: *const JavaHeapHistogram.TypeCountReader) u32 {
            return self._reachable_obj_count;
        }
        pub inline fn getSizeKb(self: *const JavaHeapHistogram.TypeCountReader) u32 {
            return self._size_kb;
        }
        pub inline fn getReachableSizeKb(self: *const JavaHeapHistogram.TypeCountReader) u32 {
            return self._reachable_size_kb;
        }
        pub inline fn getNativeSizeKb(self: *const JavaHeapHistogram.TypeCountReader) u32 {
            return self._native_size_kb;
        }
        pub inline fn getReachableNativeSizeKb(self: *const JavaHeapHistogram.TypeCountReader) u32 {
            return self._reachable_native_size_kb;
        }
    };
    const SampleWire = struct {
        const TS_WIRE: gremlin.ProtoWireNumber = 1;
        const TYPE_COUNT_WIRE: gremlin.ProtoWireNumber = 2;
    };
    pub const Sample = struct {
        // fields
        ts: i64 = 0,
        type_count: ?[]const ?JavaHeapHistogram.TypeCount = null,
        pub fn calcProtobufSize(self: *const JavaHeapHistogram.Sample) usize {
            var res: usize = 0;
            if (self.ts != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.SampleWire.TS_WIRE) + gremlin.sizes.sizeI64(self.ts);
            }
            if (self.type_count) |arr| {
                for (arr) |maybe_v| {
                    res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.SampleWire.TYPE_COUNT_WIRE);
                    if (maybe_v) |v| {
                        const size = v.calcProtobufSize();
                        res += gremlin.sizes.sizeUsize(size) + size;
                    } else {
                        res += gremlin.sizes.sizeUsize(0);
                    }
                }
            }
            return res;
        }
        pub fn encode(self: *const JavaHeapHistogram.Sample, allocator: std.mem.Allocator) gremlin.Error![]const u8 {
            const size = self.calcProtobufSize();
            if (size == 0) {
                return &[_]u8{};
            }
            const buf = try allocator.alloc(u8, self.calcProtobufSize());
            var writer = gremlin.Writer.init(buf);
            self.encodeTo(&writer);
            return buf;
        }
        pub fn encodeTo(self: *const JavaHeapHistogram.Sample, target: *gremlin.Writer) void {
            if (self.ts != 0) {
                target.appendInt64(JavaHeapHistogram.SampleWire.TS_WIRE, self.ts);
            }
            if (self.type_count) |arr| {
                for (arr) |maybe_v| {
                    if (maybe_v) |v| {
                        const size = v.calcProtobufSize();
                        target.appendBytesTag(JavaHeapHistogram.SampleWire.TYPE_COUNT_WIRE, size);
                        v.encodeTo(target);
                    } else {
                        target.appendBytesTag(JavaHeapHistogram.SampleWire.TYPE_COUNT_WIRE, 0);
                    }
                }
            }
        }
    };
    pub const SampleReader = struct {
        buf: gremlin.Reader,
        _ts: i64 = 0,
        _type_count_offset: ?usize = null,
        _type_count_last_offset: ?usize = null,
        _type_count_cnt: usize = 0,
        pub fn init(src: []const u8) gremlin.Error!JavaHeapHistogram.SampleReader {
            const buf = gremlin.Reader.init(src);
            var res = JavaHeapHistogram.SampleReader{ .buf = buf };
            if (buf.buf.len == 0) {
                return res;
            }
            var offset: usize = 0;
            while (buf.hasNext(offset, 0)) {
                const tag = try buf.readTagAt(offset);
                offset += tag.size;
                switch (tag.number) {
                    JavaHeapHistogram.SampleWire.TS_WIRE => {
                        const result = try buf.readInt64(offset);
                        offset += result.size;
                        res._ts = result.value;
                    },
                    JavaHeapHistogram.SampleWire.TYPE_COUNT_WIRE => {
                        const result = try buf.readBytes(offset);
                        offset += result.size;
                        if (res._type_count_offset == null) {
                            res._type_count_offset = offset - result.size;
                        }
                        res._type_count_last_offset = offset;
                        res._type_count_cnt += 1;
                    },
                    else => {
                        offset = try buf.skipData(offset, tag.wire);
                    },
                }
            }
            return res;
        }
        pub fn sourceBytes(self: *const @This()) []const u8 {
            return self.buf.buf;
        }
        pub inline fn getTs(self: *const JavaHeapHistogram.SampleReader) i64 {
            return self._ts;
        }
        pub fn typeCountCount(self: *const JavaHeapHistogram.SampleReader) usize {
            return self._type_count_cnt;
        }
        pub fn typeCountNext(self: *JavaHeapHistogram.SampleReader) ?JavaHeapHistogram.TypeCountReader {
            if (self._type_count_offset == null) return null;
            const current_offset = self._type_count_offset.?;
            const result = self.buf.readBytes(current_offset) catch return null;
            const msg = JavaHeapHistogram.TypeCountReader.init(result.value) catch return null;
            if (self._type_count_last_offset != null and current_offset >= self._type_count_last_offset.?) {
                self._type_count_offset = null;
                return msg;
            }
            if (self._type_count_last_offset == null) unreachable;
            var next_offset = current_offset + result.size;
            const max_offset = self._type_count_last_offset.?;
            while (next_offset <= max_offset and self.buf.hasNext(next_offset, 0)) {
                const tag = self.buf.readTagAt(next_offset) catch break;
                next_offset += tag.size;
                if (tag.number == JavaHeapHistogram.SampleWire.TYPE_COUNT_WIRE) {
                    self._type_count_offset = next_offset;
                    return msg;
                } else {
                    next_offset = self.buf.skipData(next_offset, tag.wire) catch break;
                }
            }
            self._type_count_offset = null;
            return msg;
        }
    };
    const InstanceStatsWire = struct {
        const UPID_WIRE: gremlin.ProtoWireNumber = 1;
        const PROCESS_WIRE: gremlin.ProtoWireNumber = 2;
        const SAMPLES_WIRE: gremlin.ProtoWireNumber = 3;
    };
    pub const InstanceStats = struct {
        // fields
        upid: u32 = 0,
        process: ?process_metadata.AndroidProcessMetadata = null,
        samples: ?[]const ?JavaHeapHistogram.Sample = null,
        pub fn calcProtobufSize(self: *const JavaHeapHistogram.InstanceStats) usize {
            var res: usize = 0;
            if (self.upid != 0) {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.InstanceStatsWire.UPID_WIRE) + gremlin.sizes.sizeU32(self.upid);
            }
            if (self.process) |v| {
                const size = v.calcProtobufSize();
                if (size > 0) {
                    res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.InstanceStatsWire.PROCESS_WIRE) + gremlin.sizes.sizeUsize(size) + size;
                }
            }
            if (self.samples) |arr| {
                for (arr) |maybe_v| {
                    res += gremlin.sizes.sizeWireNumber(JavaHeapHistogram.InstanceStatsWire.SAMPLES_WIRE);
                    if (maybe_v) |v| {
                        const size = v.calcProtobufSize();
                        res += gremlin.sizes.sizeUsize(size) + size;
                    } else {
                        res += gremlin.sizes.sizeUsize(0);
                    }
                }
            }
            return res;
        }
        pub fn encode(self: *const JavaHeapHistogram.InstanceStats, allocator: std.mem.Allocator) gremlin.Error![]const u8 {
            const size = self.calcProtobufSize();
            if (size == 0) {
                return &[_]u8{};
            }
            const buf = try allocator.alloc(u8, self.calcProtobufSize());
            var writer = gremlin.Writer.init(buf);
            self.encodeTo(&writer);
            return buf;
        }
        pub fn encodeTo(self: *const JavaHeapHistogram.InstanceStats, target: *gremlin.Writer) void {
            if (self.upid != 0) {
                target.appendUint32(JavaHeapHistogram.InstanceStatsWire.UPID_WIRE, self.upid);
            }
            if (self.process) |v| {
                const size = v.calcProtobufSize();
                if (size > 0) {
                    target.appendBytesTag(JavaHeapHistogram.InstanceStatsWire.PROCESS_WIRE, size);
                    v.encodeTo(target);
                }
            }
            if (self.samples) |arr| {
                for (arr) |maybe_v| {
                    if (maybe_v) |v| {
                        const size = v.calcProtobufSize();
                        target.appendBytesTag(JavaHeapHistogram.InstanceStatsWire.SAMPLES_WIRE, size);
                        v.encodeTo(target);
                    } else {
                        target.appendBytesTag(JavaHeapHistogram.InstanceStatsWire.SAMPLES_WIRE, 0);
                    }
                }
            }
        }
    };
    pub const InstanceStatsReader = struct {
        buf: gremlin.Reader,
        _upid: u32 = 0,
        _process_buf: ?[]const u8 = null,
        _samples_offset: ?usize = null,
        _samples_last_offset: ?usize = null,
        _samples_cnt: usize = 0,
        pub fn init(src: []const u8) gremlin.Error!JavaHeapHistogram.InstanceStatsReader {
            const buf = gremlin.Reader.init(src);
            var res = JavaHeapHistogram.InstanceStatsReader{ .buf = buf };
            if (buf.buf.len == 0) {
                return res;
            }
            var offset: usize = 0;
            while (buf.hasNext(offset, 0)) {
                const tag = try buf.readTagAt(offset);
                offset += tag.size;
                switch (tag.number) {
                    JavaHeapHistogram.InstanceStatsWire.UPID_WIRE => {
                        const result = try buf.readUInt32(offset);
                        offset += result.size;
                        res._upid = result.value;
                    },
                    JavaHeapHistogram.InstanceStatsWire.PROCESS_WIRE => {
                        const result = try buf.readBytes(offset);
                        offset += result.size;
                        res._process_buf = result.value;
                    },
                    JavaHeapHistogram.InstanceStatsWire.SAMPLES_WIRE => {
                        const result = try buf.readBytes(offset);
                        offset += result.size;
                        if (res._samples_offset == null) {
                            res._samples_offset = offset - result.size;
                        }
                        res._samples_last_offset = offset;
                        res._samples_cnt += 1;
                    },
                    else => {
                        offset = try buf.skipData(offset, tag.wire);
                    },
                }
            }
            return res;
        }
        pub fn sourceBytes(self: *const @This()) []const u8 {
            return self.buf.buf;
        }
        pub inline fn getUpid(self: *const JavaHeapHistogram.InstanceStatsReader) u32 {
            return self._upid;
        }
        pub fn getProcess(self: *const JavaHeapHistogram.InstanceStatsReader) gremlin.Error!process_metadata.AndroidProcessMetadataReader {
            if (self._process_buf) |buf| {
                return try process_metadata.AndroidProcessMetadataReader.init(buf);
            }
            return try process_metadata.AndroidProcessMetadataReader.init(&[_]u8{});
        }
        pub fn samplesCount(self: *const JavaHeapHistogram.InstanceStatsReader) usize {
            return self._samples_cnt;
        }
        pub fn samplesNext(self: *JavaHeapHistogram.InstanceStatsReader) ?JavaHeapHistogram.SampleReader {
            if (self._samples_offset == null) return null;
            const current_offset = self._samples_offset.?;
            const result = self.buf.readBytes(current_offset) catch return null;
            const msg = JavaHeapHistogram.SampleReader.init(result.value) catch return null;
            if (self._samples_last_offset != null and current_offset >= self._samples_last_offset.?) {
                self._samples_offset = null;
                return msg;
            }
            if (self._samples_last_offset == null) unreachable;
            var next_offset = current_offset + result.size;
            const max_offset = self._samples_last_offset.?;
            while (next_offset <= max_offset and self.buf.hasNext(next_offset, 0)) {
                const tag = self.buf.readTagAt(next_offset) catch break;
                next_offset += tag.size;
                if (tag.number == JavaHeapHistogram.InstanceStatsWire.SAMPLES_WIRE) {
                    self._samples_offset = next_offset;
                    return msg;
                } else {
                    next_offset = self.buf.skipData(next_offset, tag.wire) catch break;
                }
            }
            self._samples_offset = null;
            return msg;
        }
    };
    // fields
    instance_stats: ?[]const ?JavaHeapHistogram.InstanceStats = null,
    pub fn calcProtobufSize(self: *const JavaHeapHistogram) usize {
        var res: usize = 0;
        if (self.instance_stats) |arr| {
            for (arr) |maybe_v| {
                res += gremlin.sizes.sizeWireNumber(JavaHeapHistogramWire.INSTANCE_STATS_WIRE);
                if (maybe_v) |v| {
                    const size = v.calcProtobufSize();
                    res += gremlin.sizes.sizeUsize(size) + size;
                } else {
                    res += gremlin.sizes.sizeUsize(0);
                }
            }
        }
        return res;
    }
    pub fn encode(self: *const JavaHeapHistogram, allocator: std.mem.Allocator) gremlin.Error![]const u8 {
        const size = self.calcProtobufSize();
        if (size == 0) {
            return &[_]u8{};
        }
        const buf = try allocator.alloc(u8, self.calcProtobufSize());
        var writer = gremlin.Writer.init(buf);
        self.encodeTo(&writer);
        return buf;
    }
    pub fn encodeTo(self: *const JavaHeapHistogram, target: *gremlin.Writer) void {
        if (self.instance_stats) |arr| {
            for (arr) |maybe_v| {
                if (maybe_v) |v| {
                    const size = v.calcProtobufSize();
                    target.appendBytesTag(JavaHeapHistogramWire.INSTANCE_STATS_WIRE, size);
                    v.encodeTo(target);
                } else {
                    target.appendBytesTag(JavaHeapHistogramWire.INSTANCE_STATS_WIRE, 0);
                }
            }
        }
    }
};
pub const JavaHeapHistogramReader = struct {
    buf: gremlin.Reader,
    _instance_stats_offset: ?usize = null,
    _instance_stats_last_offset: ?usize = null,
    _instance_stats_cnt: usize = 0,
    pub fn init(src: []const u8) gremlin.Error!JavaHeapHistogramReader {
        const buf = gremlin.Reader.init(src);
        var res = JavaHeapHistogramReader{ .buf = buf };
        if (buf.buf.len == 0) {
            return res;
        }
        var offset: usize = 0;
        while (buf.hasNext(offset, 0)) {
            const tag = try buf.readTagAt(offset);
            offset += tag.size;
            switch (tag.number) {
                JavaHeapHistogramWire.INSTANCE_STATS_WIRE => {
                    const result = try buf.readBytes(offset);
                    offset += result.size;
                    if (res._instance_stats_offset == null) {
                        res._instance_stats_offset = offset - result.size;
                    }
                    res._instance_stats_last_offset = offset;
                    res._instance_stats_cnt += 1;
                },
                else => {
                    offset = try buf.skipData(offset, tag.wire);
                },
            }
        }
        return res;
    }
    pub fn sourceBytes(self: *const @This()) []const u8 {
        return self.buf.buf;
    }
    pub fn instanceStatsCount(self: *const JavaHeapHistogramReader) usize {
        return self._instance_stats_cnt;
    }
    pub fn instanceStatsNext(self: *JavaHeapHistogramReader) ?JavaHeapHistogram.InstanceStatsReader {
        if (self._instance_stats_offset == null) return null;
        const current_offset = self._instance_stats_offset.?;
        const result = self.buf.readBytes(current_offset) catch return null;
        const msg = JavaHeapHistogram.InstanceStatsReader.init(result.value) catch return null;
        if (self._instance_stats_last_offset != null and current_offset >= self._instance_stats_last_offset.?) {
            self._instance_stats_offset = null;
            return msg;
        }
        if (self._instance_stats_last_offset == null) unreachable;
        var next_offset = current_offset + result.size;
        const max_offset = self._instance_stats_last_offset.?;
        while (next_offset <= max_offset and self.buf.hasNext(next_offset, 0)) {
            const tag = self.buf.readTagAt(next_offset) catch break;
            next_offset += tag.size;
            if (tag.number == JavaHeapHistogramWire.INSTANCE_STATS_WIRE) {
                self._instance_stats_offset = next_offset;
                return msg;
            } else {
                next_offset = self.buf.skipData(next_offset, tag.wire) catch break;
            }
        }
        self._instance_stats_offset = null;
        return msg;
    }
};
